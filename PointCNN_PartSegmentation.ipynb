{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PointCNN_PartSegmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phongnd205/PointCNN_Face_Part_Segmentation/blob/main/PointCNN_PartSegmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "655yBtxbJksQ",
        "outputId": "22fa8e38-2a9c-457f-ee12-1661f7482747"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hijZaonhKJea"
      },
      "source": [
        "#!git clone https://github.com/yangyanli/PointCNN.git '/content/drive/MyDrive/Colab_Notebooks/PointCNN'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQE2klFrE2xm",
        "outputId": "cce0a9ea-a8c7-47b3-e30f-3ff4d8cec3ac"
      },
      "source": [
        "%tensorflow_version 1.0\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.0`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY_KXVXh3U-i"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XLvoJ96E5oS"
      },
      "source": [
        "ls /tensorflow-1.15.2/python3.7/tensorflow_core"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb8Fbo6gIYAd"
      },
      "source": [
        "cd '/tensorflow-1.15.2/python3.7/tensorflow_core'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCaEBT8LIZpt"
      },
      "source": [
        "!sudo ln -s libtensorflow_framework.so.1 libtensorflow_framework.so"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OL5zcl2HIbOZ"
      },
      "source": [
        "cp /tensorflow-1.15.2/python3.7/tensorflow_core/libtensorflow_framework.so /usr/lib/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKt3gf6zIcrB"
      },
      "source": [
        "cp /tensorflow-1.15.2/python3.7/tensorflow_core/libtensorflow_framework.so.1 /usr/lib/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzzxiSGEEZP7"
      },
      "source": [
        "cd '/content/drive/MyDrive/Colab_Notebooks/PointCNN/sampling'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFysifUzDYQ-"
      },
      "source": [
        "!bash tf_sampling_compile.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yBCM3inJoxO",
        "outputId": "2ced0b95-be08-43c4-9daf-207b3952534b"
      },
      "source": [
        "cd '/content/drive/MyDrive/Colab_Notebooks/PointCNN'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1cHCgIPih8muLnTVO67gyq4Ml30OLbACX/PointCNN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gp1jEUdNKaLi"
      },
      "source": [
        "pip install plyfile transforms3d tqdm svgpathtools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JzI6WWe_AGq"
      },
      "source": [
        "#!/usr/bin/python3\n",
        "\"\"\"Training and Validation On Segmentation Task.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import random\n",
        "import shutil\n",
        "import argparse\n",
        "import importlib\n",
        "import data_utils\n",
        "import numpy as np\n",
        "import pointfly as pf\n",
        "import tensorflow as tf\n",
        "from datetime import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5y5Vh6H2LBj9"
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "#parser.add_argument('--filelist', '-t', help='Path to training set ground truth (.txt)', required=True)\n",
        "#parser.add_argument('--filelist_val', '-v', help='Path to validation set ground truth (.txt)', required=True)\n",
        "parser.add_argument('--load_ckpt', '-l', help='Path to a check point file for load')\n",
        "parser.add_argument('--save_folder', default='log2/Segmentation', help='Path to folder for saving check points and summary')\n",
        "parser.add_argument('--model', default='pointcnn_seg2', help='Model to use')\n",
        "parser.add_argument('--setting', help='Setting to use')\n",
        "parser.add_argument('--epochs', type=int, default=2048, help='Number of training epochs (default defined in setting)')\n",
        "parser.add_argument('--batch_size', type=int, default=8, help='Batch size (default defined in setting)')\n",
        "parser.add_argument('--learning_rate', type=float, default=0.001, help='Initial learning rate [default: 0.001]')\n",
        "parser.add_argument('--log', help='Log to FILE in save folder; use - for stdout (default is log.txt)', metavar='FILE', default='log.txt')\n",
        "parser.add_argument('--no_timestamp_folder', help='Dont save to timestamp folder', action='store_true')\n",
        "parser.add_argument('--no_code_backup', help='Dont backup code', action='store_true')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Znte3IWE_Q6K"
      },
      "source": [
        "args = parser.parse_args(args=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZ0Sl-qOhAWo"
      },
      "source": [
        "class setting:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGIqA9Srkqux"
      },
      "source": [
        "Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UIDPUO3WgBM"
      },
      "source": [
        "setting.num_class = 50\n",
        "\n",
        "setting.sample_num = 2048\n",
        "\n",
        "setting.batch_size = 8\n",
        "\n",
        "setting.num_epochs = 401\n",
        "\n",
        "setting.label_weights = []\n",
        "\n",
        "setting.label_weights = [1.0] * setting.num_class\n",
        "\n",
        "setting.learning_rate_base = 0.001\n",
        "setting.decay_steps = 100000\n",
        "setting.decay_rate = 0.35\n",
        "setting.learning_rate_min = 0.00001\n",
        "setting.step_val = 2000\n",
        "\n",
        "setting.weight_decay = 0.0\n",
        "\n",
        "setting.jitter = 0.001\n",
        "setting.jitter_val = 0.0\n",
        "\n",
        "setting.rotation_range = [0, 0, 0, 'u']\n",
        "setting.rotation_range_val = [0, 0, 0, 'u']\n",
        "setting.rotation_order = 'rxyz'\n",
        "\n",
        "setting.scaling_range = [0.0, 0.0, 0.0, 'g']\n",
        "setting.scaling_range_val = [0, 0, 0, 'u']\n",
        "\n",
        "setting.sample_num_variance = 1 // 8\n",
        "setting.sample_num_clip = 1 // 4\n",
        "\n",
        "setting.x = 8\n",
        "x = setting.x\n",
        "setting.xconv_param_name = ('K', 'D', 'P', 'C', 'links')\n",
        "setting.xconv_params = [dict(zip(setting.xconv_param_name, xconv_param)) for xconv_param in\n",
        "                [(8, 1, -1, 32 * x, []),\n",
        "                 (12, 2, 768, 32 * x, []),\n",
        "                 (16, 2, 384, 64 * x, []),\n",
        "                 (16, 6, 128, 128 * x, [])]]\n",
        "\n",
        "setting.with_global = True\n",
        "\n",
        "setting.xdconv_param_name = ('K', 'D', 'pts_layer_idx', 'qrs_layer_idx')\n",
        "setting.xdconv_params = [dict(zip(setting.xdconv_param_name, xdconv_param)) for xdconv_param in\n",
        "                 [(16, 6, 3, 3),\n",
        "                  (16, 6, 3, 2),\n",
        "                  (12, 6, 2, 1),\n",
        "                  (8, 6, 1, 0),\n",
        "                  (8, 4, 0, 0)]]\n",
        "\n",
        "setting.fc_param_name = ('C', 'dropout_rate')\n",
        "setting.fc_params = [dict(zip(setting.fc_param_name, fc_param)) for fc_param in\n",
        "             [(32 * x, 0.0),\n",
        "              (32 * x, 0.5)]]\n",
        "\n",
        "setting.sampling = 'fps'\n",
        "\n",
        "setting.optimizer = 'adam'\n",
        "setting.epsilon = 1e-3\n",
        "setting.momentum = 0.9\n",
        "\n",
        "setting.data_dim = 3\n",
        "setting.with_X_transformation = True\n",
        "setting.sorting_method = None\n",
        "\n",
        "setting.keep_remainder = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZL6wQxCYgZZ"
      },
      "source": [
        "model = importlib.import_module(args.model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgQ9Kr0l_dhB"
      },
      "source": [
        "label_weights_list = setting.label_weights\n",
        "num_epochs = setting.num_epochs\n",
        "batch_size = setting.batch_size\n",
        "step_val = setting.step_val\n",
        "sample_num = setting.sample_num\n",
        "rotation_range = setting.rotation_range,\n",
        "scaling_range = setting.scaling_range,\n",
        "\n",
        "jitter = setting.jitter\n",
        "jitter_val = setting.jitter_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKmjXLVxufbw"
      },
      "source": [
        "rotation_range_val = setting.rotation_range_val\n",
        "scaling_range_val = setting.scaling_range_val\n",
        "scaling_range = setting.scaling_range"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "mWKlLm6DTWcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQlDyP3kEnIW"
      },
      "source": [
        "point_data = np.load('/content/drive/MyDrive/Colab_Notebooks/PointCNN/PartSegmentationData_data.npy')\n",
        "point_label = np.load('/content/drive/MyDrive/Colab_Notebooks/PointCNN/PartSegmentationData_label.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = np.arange(0,666)\n",
        "index = index*4\n",
        "point_segment_data_label = {\"data\": point_data[index,:,:], \"label\": point_label[index,:]}"
      ],
      "metadata": {
        "id": "GvnmpoCZWMn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haZMP8X4c5YG"
      },
      "source": [
        "data_num = np.full((2664), 4096)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PkPGy6vp9fn"
      },
      "source": [
        "test_index = np.arange(666*2,666*3)\n",
        "for_train_index = np.array([i for i in range(len(point_data)) if i not in test_index])\n",
        "#np.random.shuffle(for_train_index)\n",
        "train_index = for_train_index[0:1920]\n",
        "valid_index = for_train_index[1920:1998]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyLTHMZarbA9",
        "outputId": "4acad3f7-682c-443b-d2ed-4682117b90d9"
      },
      "source": [
        "666*4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2664"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfwWjf9LeGyu"
      },
      "source": [
        "Training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsVx6L-ppy-N"
      },
      "source": [
        "data_train = point_data[train_index,0:4096,:]\n",
        "data_num_train = data_num[train_index,]\n",
        "label_train = point_label[train_index,0:4096]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi3SVYJZeJ12"
      },
      "source": [
        "Validate data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPL3jbvTeJE2"
      },
      "source": [
        "data_val = point_data[valid_index,0:4096,:]\n",
        "data_num_val = data_num[valid_index,]\n",
        "label_val = point_label[valid_index,0:4096]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARUN3eG6o3ZG"
      },
      "source": [
        "del point_data\n",
        "del point_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed6Qka4Q_mcO"
      },
      "source": [
        "# shuffle\n",
        "data_train, data_num_train, label_train = \\\n",
        "    data_utils.grouped_shuffle([data_train, data_num_train, label_train])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lfd49lTtfMXt"
      },
      "source": [
        "num_train = data_train.shape[0]\n",
        "point_num = data_train.shape[1]\n",
        "num_val = data_val.shape[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vl6k4ggq0pb"
      },
      "source": [
        "batch_num = (num_train * num_epochs + batch_size - 1) // batch_size\n",
        "batch_num_val = math.ceil(num_val / batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5H1UrC4_oFp"
      },
      "source": [
        "######################################################################\n",
        "# Placeholders\n",
        "indices = tf.placeholder(tf.int32, shape=(None, None, 2), name=\"indices\")\n",
        "xforms = tf.placeholder(tf.float32, shape=(None, 3, 3), name=\"xforms\")\n",
        "rotations = tf.placeholder(tf.float32, shape=(None, 3, 3), name=\"rotations\")\n",
        "jitter_range = tf.placeholder(tf.float32, shape=(1), name=\"jitter_range\")\n",
        "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
        "is_training = tf.placeholder(tf.bool, name='is_training')\n",
        "\n",
        "pts_fts = tf.placeholder(tf.float32, shape=(None, point_num, setting.data_dim), name='pts_fts')\n",
        "labels_seg = tf.placeholder(tf.int64, shape=(None, point_num), name='labels_seg')\n",
        "labels_weights = tf.placeholder(tf.float32, shape=(None, point_num), name='labels_weights')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_twF1m4_vKi"
      },
      "source": [
        "######################################################################\n",
        "pts_fts_sampled = tf.gather_nd(pts_fts, indices=indices, name='pts_fts_sampled')\n",
        "features_augmented = None\n",
        "if setting.data_dim > 3:\n",
        "    points_sampled, features_sampled = tf.split(pts_fts_sampled,\n",
        "                                                [3, setting.data_dim - 3],\n",
        "                                                axis=-1,\n",
        "                                                name='split_points_features')\n",
        "    if setting.use_extra_features:\n",
        "        if setting.with_normal_feature:\n",
        "            if setting.data_dim < 6:\n",
        "                print('Only 3D normals are supported!')\n",
        "                exit()\n",
        "            elif setting.data_dim == 6:\n",
        "                features_augmented = pf.augment(features_sampled, rotations)\n",
        "            else:\n",
        "                normals, rest = tf.split(features_sampled, [3, setting.data_dim - 6])\n",
        "                normals_augmented = pf.augment(normals, rotations)\n",
        "                features_augmented = tf.concat([normals_augmented, rest], axis=-1)\n",
        "        else:\n",
        "            features_augmented = features_sampled\n",
        "else:\n",
        "    points_sampled = pts_fts_sampled\n",
        "points_augmented = pf.augment(points_sampled, xforms, jitter_range)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp2IFu0K_09q"
      },
      "source": [
        "labels_sampled = tf.gather_nd(labels_seg, indices=indices, name='labels_sampled')\n",
        "labels_weights_sampled = tf.gather_nd(labels_weights, indices=indices, name='labels_weight_sampled')\n",
        "\n",
        "net = model.Net(points_augmented, features_augmented, is_training, setting)\n",
        "logits = net.logits\n",
        "probs = tf.nn.softmax(logits, name='probs')\n",
        "predictions = tf.argmax(probs, axis=-1, name='predictions')\n",
        "\n",
        "loss_op = tf.losses.sparse_softmax_cross_entropy(labels=labels_sampled, logits=logits,\n",
        "                                                 weights=labels_weights_sampled)\n",
        "\n",
        "with tf.name_scope('metrics'):\n",
        "    loss_mean_op, loss_mean_update_op = tf.metrics.mean(loss_op)\n",
        "    t_1_acc_op, t_1_acc_update_op = tf.metrics.accuracy(labels_sampled, predictions, weights=labels_weights_sampled)\n",
        "    t_1_per_class_acc_op, t_1_per_class_acc_update_op = \\\n",
        "        tf.metrics.mean_per_class_accuracy(labels_sampled, predictions, setting.num_class,\n",
        "                                           weights=labels_weights_sampled)\n",
        "reset_metrics_op = tf.variables_initializer([var for var in tf.local_variables()\n",
        "                                             if var.name.split('/')[0] == 'metrics'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjbIf19HAAY5"
      },
      "source": [
        "_ = tf.summary.scalar('loss/train', tensor=loss_mean_op, collections=['train'])\n",
        "_ = tf.summary.scalar('t_1_acc/train', tensor=t_1_acc_op, collections=['train'])\n",
        "_ = tf.summary.scalar('t_1_per_class_acc/train', tensor=t_1_per_class_acc_op, collections=['train'])\n",
        "\n",
        "_ = tf.summary.scalar('loss/val', tensor=loss_mean_op, collections=['val'])\n",
        "_ = tf.summary.scalar('t_1_acc/val', tensor=t_1_acc_op, collections=['val'])\n",
        "_ = tf.summary.scalar('t_1_per_class_acc/val', tensor=t_1_per_class_acc_op, collections=['val'])\n",
        "\n",
        "lr_exp_op = tf.train.exponential_decay(setting.learning_rate_base, global_step, setting.decay_steps,\n",
        "                                       setting.decay_rate, staircase=True)\n",
        "lr_clip_op = tf.maximum(lr_exp_op, setting.learning_rate_min)\n",
        "_ = tf.summary.scalar('learning_rate', tensor=lr_clip_op, collections=['train'])\n",
        "reg_loss = setting.weight_decay * tf.losses.get_regularization_loss()\n",
        "if setting.optimizer == 'adam':\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=lr_clip_op, epsilon=setting.epsilon)\n",
        "elif setting.optimizer == 'momentum':\n",
        "    optimizer = tf.train.MomentumOptimizer(learning_rate=lr_clip_op, momentum=setting.momentum, use_nesterov=True)\n",
        "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "with tf.control_dependencies(update_ops):\n",
        "    train_op = optimizer.minimize(loss_op + reg_loss, global_step=global_step)\n",
        "\n",
        "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
        "\n",
        "saver = tf.train.Saver(max_to_keep=None)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8LIflP4gF0z"
      },
      "source": [
        "root_folder = '/content/drive/MyDrive/Colab_Notebooks/PointCNN/Train/npoint_4096_batch_8_epoch_401_learn_001'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOYRt2-Of-rb"
      },
      "source": [
        "folder_ckpt = os.path.join(root_folder, 'ckpts_fold_3')\n",
        "if not os.path.exists(folder_ckpt):\n",
        "    os.makedirs(folder_ckpt)\n",
        "\n",
        "folder_summary = os.path.join(root_folder, 'summary')\n",
        "if not os.path.exists(folder_summary):\n",
        "    os.makedirs(folder_summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "di8ffwoLAGWh",
        "outputId": "502e784a-325e-4851-c5ff-03224ae6d209"
      },
      "source": [
        "parameter_num = np.sum([np.prod(v.shape.as_list()) for v in tf.trainable_variables()])\n",
        "print('{}-Parameter number: {:d}.'.format(datetime.now(), parameter_num))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    summaries_op = tf.summary.merge_all('train')\n",
        "    summaries_val_op = tf.summary.merge_all('val')\n",
        "    summary_writer = tf.summary.FileWriter(folder_summary, sess.graph)\n",
        "\n",
        "    sess.run(init_op)\n",
        "\n",
        "    # Load the model\n",
        "    if args.load_ckpt is not None:\n",
        "        saver.restore(sess, args.load_ckpt)\n",
        "        print('{}-Checkpoint loaded from {}!'.format(datetime.now(), args.load_ckpt))\n",
        "    else:\n",
        "        latest_ckpt = tf.train.latest_checkpoint(folder_ckpt)\n",
        "        #latest_ckpt = '/content/drive/MyDrive/Colab_Notebooks/PointCNN/Train/npoint_2048_batch_16_epoch_401_learn_01_decay_100000_decaterate_035/ckpts_fold_2/iter-14500'\n",
        "\n",
        "        if latest_ckpt:\n",
        "            print('{}-Found checkpoint {}'.format(datetime.now(), latest_ckpt))\n",
        "            saver.restore(sess, latest_ckpt)\n",
        "            print('{}-Checkpoint loaded from {} (Iter {})'.format(\n",
        "                datetime.now(), latest_ckpt, sess.run(global_step)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-11-22 12:25:49.619765-Parameter number: 8326770.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SpPhQyOmOJt"
      },
      "source": [
        "\n",
        "\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transforms3d.euler import euler2mat\n",
        "\n",
        "\n",
        "# the returned indices will be used by tf.gather_nd\n",
        "def get_indices(batch_size, sample_num, point_num, pool_setting=None):\n",
        "    if not isinstance(point_num, np.ndarray):\n",
        "        point_nums = np.full((batch_size), point_num)\n",
        "    else:\n",
        "        point_nums = point_num\n",
        "\n",
        "    indices = []\n",
        "    for i in range(batch_size):\n",
        "        pt_num = point_nums[i]\n",
        "        if pool_setting is None:\n",
        "            pool_size = pt_num\n",
        "        else:\n",
        "            if isinstance(pool_setting, int):\n",
        "                pool_size = min(pool_setting, pt_num)\n",
        "            elif isinstance(pool_setting, tuple):\n",
        "                pool_size = min(random.randrange(pool_setting[0], pool_setting[1]+1), pt_num)\n",
        "        if pool_size > sample_num:\n",
        "            choices = np.random.choice(pool_size, sample_num, replace=False)\n",
        "        else:\n",
        "            choices = np.concatenate((np.random.choice(pool_size, pool_size, replace=False),\n",
        "                                      np.random.choice(pool_size, sample_num - pool_size, replace=True)))\n",
        "        if pool_size < pt_num:\n",
        "            choices_pool = np.random.choice(pt_num, pool_size, replace=False)\n",
        "            choices = choices_pool[choices]\n",
        "        choices = np.expand_dims(choices, axis=1)\n",
        "        choices_2d = np.concatenate((np.full_like(choices, i), choices), axis=1)\n",
        "        indices.append(choices_2d)\n",
        "    return np.stack(indices)\n",
        "\n",
        "\n",
        "def gauss_clip(mu, sigma, clip):\n",
        "    v = random.gauss(mu, sigma)\n",
        "    v = max(min(v, mu + clip * sigma), mu - clip * sigma)\n",
        "    return v\n",
        "\n",
        "\n",
        "def uniform(bound):\n",
        "    return bound * (2 * random.random() - 1)\n",
        "\n",
        "\n",
        "def scaling_factor(scaling_param, method):\n",
        "    try:\n",
        "        scaling_list = list(scaling_param)\n",
        "        return random.choice(scaling_list)\n",
        "    except:\n",
        "        if method == 'g':\n",
        "            return gauss_clip(1.0, scaling_param, 3)\n",
        "        elif method == 'u':\n",
        "            return 1.0 + uniform(scaling_param)\n",
        "\n",
        "\n",
        "def rotation_angle(rotation_param, method):\n",
        "    try:\n",
        "        rotation_list = list(rotation_param)\n",
        "        return random.choice(rotation_list)\n",
        "    except:\n",
        "        if method == 'g':\n",
        "            return gauss_clip(0.0, rotation_param, 3)\n",
        "        elif method == 'u':\n",
        "            return uniform(rotation_param)\n",
        "\n",
        "\n",
        "def get_xforms(xform_num, rotation_range, scaling_range, order='rxyz'):\n",
        "    xforms = np.empty(shape=(xform_num, 3, 3))\n",
        "    rotations = np.empty(shape=(xform_num, 3, 3))\n",
        "    for i in range(xform_num):\n",
        "        rx = rotation_angle(rotation_range[0][0], rotation_range[0][3])\n",
        "        ry = rotation_angle(rotation_range[0][1], rotation_range[0][3])\n",
        "        rz = rotation_angle(rotation_range[0][2], rotation_range[0][3])\n",
        "        rotation = euler2mat(rx, ry, rz, order)\n",
        "\n",
        "        sx = scaling_factor(scaling_range[0][0], scaling_range[0][3])\n",
        "        sy = scaling_factor(scaling_range[0][1], scaling_range[0][3])\n",
        "        sz = scaling_factor(scaling_range[0][2], scaling_range[0][3])\n",
        "        scaling = np.diag([sx, sy, sz])\n",
        "\n",
        "        xforms[i, :] = scaling * rotation\n",
        "        rotations[i, :] = rotation\n",
        "    return xforms, rotations\n",
        "\n",
        "\n",
        "def augment(points, xforms, range=None):\n",
        "    points_xformed = tf.matmul(points, xforms, name='points_xformed')\n",
        "    if range is None:\n",
        "        return points_xformed\n",
        "\n",
        "    jitter_data = range * tf.random_normal(tf.shape(points_xformed), name='jitter_data')\n",
        "    jitter_clipped = tf.clip_by_value(jitter_data, -5 * range, 5 * range, name='jitter_clipped')\n",
        "    return points_xformed + jitter_clipped\n",
        "\n",
        "\n",
        "# A shape is (N, C)\n",
        "def distance_matrix(A):\n",
        "    r = tf.reduce_sum(A * A, 1, keep_dims=True)\n",
        "    m = tf.matmul(A, tf.transpose(A))\n",
        "    D = r - 2 * m + tf.transpose(r)\n",
        "    return D\n",
        "\n",
        "\n",
        "# A shape is (N, P, C)\n",
        "def batch_distance_matrix(A):\n",
        "    r = tf.reduce_sum(A * A, axis=2, keep_dims=True)\n",
        "    m = tf.matmul(A, tf.transpose(A, perm=(0, 2, 1)))\n",
        "    D = r - 2 * m + tf.transpose(r, perm=(0, 2, 1))\n",
        "    return D\n",
        "\n",
        "\n",
        "# A shape is (N, P_A, C), B shape is (N, P_B, C)\n",
        "# D shape is (N, P_A, P_B)\n",
        "def batch_distance_matrix_general(A, B):\n",
        "    r_A = tf.reduce_sum(A * A, axis=2, keep_dims=True)\n",
        "    r_B = tf.reduce_sum(B * B, axis=2, keep_dims=True)\n",
        "    m = tf.matmul(A, tf.transpose(B, perm=(0, 2, 1)))\n",
        "    D = r_A - 2 * m + tf.transpose(r_B, perm=(0, 2, 1))\n",
        "    return D\n",
        "\n",
        "\n",
        "# A shape is (N, P, C)\n",
        "def find_duplicate_columns(A):\n",
        "    N = A.shape[0]\n",
        "    P = A.shape[1]\n",
        "    indices_duplicated = np.fill((N, 1, P), 1, dtype=np.int32)\n",
        "    for idx in range(N):\n",
        "        _, indices = np.unique(A[idx], return_index=True, axis=0)\n",
        "        indices_duplicated[idx, :, indices] = 0\n",
        "    return indices_duplicated\n",
        "\n",
        "\n",
        "# add a big value to duplicate columns\n",
        "def prepare_for_unique_top_k(D, A):\n",
        "    indices_duplicated = tf.py_func(find_duplicate_columns, [A], tf.int32)\n",
        "    D += tf.reduce_max(D)*tf.cast(indices_duplicated, tf.float32)\n",
        "\n",
        "\n",
        "# return shape is (N, P, K, 2)\n",
        "def knn_indices(points, k, sort=True, unique=True):\n",
        "    points_shape = tf.shape(points)\n",
        "    batch_size = points_shape[0]\n",
        "    point_num = points_shape[1]\n",
        "\n",
        "    D = batch_distance_matrix(points)\n",
        "    if unique:\n",
        "        prepare_for_unique_top_k(D, points)\n",
        "    distances, point_indices = tf.nn.top_k(-D, k=k, sorted=sort)\n",
        "    batch_indices = tf.tile(tf.reshape(tf.range(batch_size), (-1, 1, 1, 1)), (1, point_num, k, 1))\n",
        "    indices = tf.concat([batch_indices, tf.expand_dims(point_indices, axis=3)], axis=3)\n",
        "    return -distances, indices\n",
        "\n",
        "\n",
        "# return shape is (N, P, K, 2)\n",
        "def knn_indices_general(queries, points, k, sort=True, unique=True):\n",
        "    queries_shape = tf.shape(queries)\n",
        "    batch_size = queries_shape[0]\n",
        "    point_num = queries_shape[1]\n",
        "\n",
        "    D = batch_distance_matrix_general(queries, points)\n",
        "    if unique:\n",
        "        prepare_for_unique_top_k(D, points)\n",
        "    distances, point_indices = tf.nn.top_k(-D, k=k, sorted=sort)  # (N, P, K)\n",
        "    batch_indices = tf.tile(tf.reshape(tf.range(batch_size), (-1, 1, 1, 1)), (1, point_num, k, 1))\n",
        "    indices = tf.concat([batch_indices, tf.expand_dims(point_indices, axis=3)], axis=3)\n",
        "    return -distances, indices\n",
        "\n",
        "\n",
        "# indices is (N, P, K, 2)\n",
        "# return shape is (N, P, K, 2)\n",
        "def sort_points(points, indices, sorting_method):\n",
        "    indices_shape = tf.shape(indices)\n",
        "    batch_size = indices_shape[0]\n",
        "    point_num = indices_shape[1]\n",
        "    k = indices_shape[2]\n",
        "\n",
        "    nn_pts = tf.gather_nd(points, indices)  # (N, P, K, 3)\n",
        "    if sorting_method.startswith('c'):\n",
        "        if ''.join(sorted(sorting_method[1:])) != 'xyz':\n",
        "            print('Unknown sorting method!')\n",
        "            exit()\n",
        "        epsilon = 1e-8\n",
        "        nn_pts_min = tf.reduce_min(nn_pts, axis=2, keep_dims=True)\n",
        "        nn_pts_max = tf.reduce_max(nn_pts, axis=2, keep_dims=True)\n",
        "        nn_pts_normalized = (nn_pts - nn_pts_min) / (nn_pts_max - nn_pts_min + epsilon)  # (N, P, K, 3)\n",
        "        scaling_factors = [math.pow(100.0, 3 - sorting_method.find('x')),\n",
        "                           math.pow(100.0, 3 - sorting_method.find('y')),\n",
        "                           math.pow(100.0, 3 - sorting_method.find('z'))]\n",
        "        scaling = tf.constant(scaling_factors, shape=(1, 1, 1, 3))\n",
        "        sorting_data = tf.reduce_sum(nn_pts_normalized * scaling, axis=-1)  # (N, P, K)\n",
        "        sorting_data = tf.concat([tf.zeros((batch_size, point_num, 1)), sorting_data[:, :, 1:]], axis=-1)\n",
        "    elif sorting_method == 'l2':\n",
        "        nn_pts_center = tf.reduce_mean(nn_pts, axis=2, keep_dims=True)  # (N, P, 1, 3)\n",
        "        nn_pts_local = tf.subtract(nn_pts, nn_pts_center)  # (N, P, K, 3)\n",
        "        sorting_data = tf.norm(nn_pts_local, axis=-1)  # (N, P, K)\n",
        "    else:\n",
        "        print('Unknown sorting method!')\n",
        "        exit()\n",
        "    _, k_indices = tf.nn.top_k(sorting_data, k=k, sorted=True)  # (N, P, K)\n",
        "    batch_indices = tf.tile(tf.reshape(tf.range(batch_size), (-1, 1, 1, 1)), (1, point_num, k, 1))\n",
        "    point_indices = tf.tile(tf.reshape(tf.range(point_num), (1, -1, 1, 1)), (batch_size, 1, k, 1))\n",
        "    k_indices_4d = tf.expand_dims(k_indices, axis=3)\n",
        "    sorting_indices = tf.concat([batch_indices, point_indices, k_indices_4d], axis=3)  # (N, P, K, 3)\n",
        "    return tf.gather_nd(indices, sorting_indices)\n",
        "\n",
        "\n",
        "# a b c\n",
        "# d e f\n",
        "# g h i\n",
        "# a(ei − fh) − b(di − fg) + c(dh − eg)\n",
        "def compute_determinant(A):\n",
        "    return A[..., 0, 0] * (A[..., 1, 1] * A[..., 2, 2] - A[..., 1, 2] * A[..., 2, 1]) \\\n",
        "           - A[..., 0, 1] * (A[..., 1, 0] * A[..., 2, 2] - A[..., 1, 2] * A[..., 2, 0]) \\\n",
        "           + A[..., 0, 2] * (A[..., 1, 0] * A[..., 2, 1] - A[..., 1, 1] * A[..., 2, 0])\n",
        "\n",
        "\n",
        "# A shape is (N, P, 3, 3)\n",
        "# return shape is (N, P, 3)\n",
        "def compute_eigenvals(A):\n",
        "    A_11 = A[:, :, 0, 0]  # (N, P)\n",
        "    A_12 = A[:, :, 0, 1]\n",
        "    A_13 = A[:, :, 0, 2]\n",
        "    A_22 = A[:, :, 1, 1]\n",
        "    A_23 = A[:, :, 1, 2]\n",
        "    A_33 = A[:, :, 2, 2]\n",
        "    I = tf.eye(3)\n",
        "    p1 = tf.square(A_12) + tf.square(A_13) + tf.square(A_23)  # (N, P)\n",
        "    q = tf.trace(A) / 3  # (N, P)\n",
        "    p2 = tf.square(A_11 - q) + tf.square(A_22 - q) + tf.square(A_33 - q) + 2 * p1  # (N, P)\n",
        "    p = tf.sqrt(p2 / 6) + 1e-8  # (N, P)\n",
        "    N = tf.shape(A)[0]\n",
        "    q_4d = tf.reshape(q, (N, -1, 1, 1))  # (N, P, 1, 1)\n",
        "    p_4d = tf.reshape(p, (N, -1, 1, 1))\n",
        "    B = (1 / p_4d) * (A - q_4d * I)  # (N, P, 3, 3)\n",
        "    r = tf.clip_by_value(compute_determinant(B) / 2, -1, 1)  # (N, P)\n",
        "    phi = tf.acos(r) / 3  # (N, P)\n",
        "    eig1 = q + 2 * p * tf.cos(phi)  # (N, P)\n",
        "    eig3 = q + 2 * p * tf.cos(phi + (2 * math.pi / 3))\n",
        "    eig2 = 3 * q - eig1 - eig3\n",
        "    return tf.abs(tf.stack([eig1, eig2, eig3], axis=2))  # (N, P, 3)\n",
        "\n",
        "\n",
        "# P shape is (N, P, 3), N shape is (N, P, K, 3)\n",
        "# return shape is (N, P)\n",
        "def compute_curvature(nn_pts):\n",
        "    nn_pts_mean = tf.reduce_mean(nn_pts, axis=2, keep_dims=True)  # (N, P, 1, 3)\n",
        "    nn_pts_demean = nn_pts - nn_pts_mean  # (N, P, K, 3)\n",
        "    nn_pts_NPK31 = tf.expand_dims(nn_pts_demean, axis=-1)\n",
        "    covariance_matrix = tf.matmul(nn_pts_NPK31, nn_pts_NPK31, transpose_b=True)  # (N, P, K, 3, 3)\n",
        "    covariance_matrix_mean = tf.reduce_mean(covariance_matrix, axis=2)  # (N, P, 3, 3)\n",
        "    eigvals = compute_eigenvals(covariance_matrix_mean)  # (N, P, 3)\n",
        "    curvature = tf.reduce_min(eigvals, axis=-1) / (tf.reduce_sum(eigvals, axis=-1) + 1e-8)\n",
        "    return curvature\n",
        "\n",
        "\n",
        "def curvature_based_sample(nn_pts, k):\n",
        "    curvature = compute_curvature(nn_pts)\n",
        "    _, point_indices = tf.nn.top_k(curvature, k=k, sorted=False)\n",
        "\n",
        "    pts_shape = tf.shape(nn_pts)\n",
        "    batch_size = pts_shape[0]\n",
        "    batch_indices = tf.tile(tf.reshape(tf.range(batch_size), (-1, 1, 1)), (1, k, 1))\n",
        "    indices = tf.concat([batch_indices, tf.expand_dims(point_indices, axis=2)], axis=2)\n",
        "    return indices\n",
        "\n",
        "\n",
        "def random_choice_2d(size, prob_matrix):\n",
        "    n_row = prob_matrix.shape[0]\n",
        "    n_col = prob_matrix.shape[1]\n",
        "    choices = np.ones((n_row, size), dtype=np.int32)\n",
        "    for idx_row in range(n_row):\n",
        "        choices[idx_row] = np.random.choice(n_col, size=size, replace=False, p=prob_matrix[idx_row])\n",
        "    return choices\n",
        "\n",
        "\n",
        "def inverse_density_sampling(points, k, sample_num):\n",
        "    D = batch_distance_matrix(points)\n",
        "    distances, _ = tf.nn.top_k(-D, k=k, sorted=False)\n",
        "    distances_avg = tf.abs(tf.reduce_mean(distances, axis=-1)) + 1e-8\n",
        "    prob_matrix = distances_avg / tf.reduce_sum(distances_avg, axis=-1, keep_dims=True)\n",
        "    point_indices = tf.py_func(random_choice_2d, [sample_num, prob_matrix], tf.int32)\n",
        "    point_indices.set_shape([points.get_shape()[0], sample_num])\n",
        "\n",
        "    batch_size = tf.shape(points)[0]\n",
        "    batch_indices = tf.tile(tf.reshape(tf.range(batch_size), (-1, 1, 1)), (1, sample_num, 1))\n",
        "    indices = tf.concat([batch_indices, tf.expand_dims(point_indices, axis=2)], axis=2)\n",
        "    return indices\n",
        "\n",
        "\n",
        "def batch_normalization(data, is_training, name, reuse=None):\n",
        "    return tf.layers.batch_normalization(data, momentum=0.99, training=is_training,\n",
        "                                         beta_regularizer=tf.contrib.layers.l2_regularizer(scale=1.0),\n",
        "                                         gamma_regularizer=tf.contrib.layers.l2_regularizer(scale=1.0),\n",
        "                                         reuse=reuse, name=name)\n",
        "\n",
        "\n",
        "def separable_conv2d(input, output, name, is_training, kernel_size, depth_multiplier=1,\n",
        "                     reuse=None, with_bn=True, activation=tf.nn.elu):\n",
        "    conv2d = tf.layers.separable_conv2d(input, output, kernel_size=kernel_size, strides=(1, 1), padding='VALID',\n",
        "                                        activation=activation,\n",
        "                                        depth_multiplier=depth_multiplier,\n",
        "                                        depthwise_initializer=tf.glorot_normal_initializer(),\n",
        "                                        pointwise_initializer=tf.glorot_normal_initializer(),\n",
        "                                        depthwise_regularizer=tf.contrib.layers.l2_regularizer(scale=1.0),\n",
        "                                        pointwise_regularizer=tf.contrib.layers.l2_regularizer(scale=1.0),\n",
        "                                        reuse=reuse, name=name, use_bias=not with_bn)\n",
        "    return batch_normalization(conv2d, is_training, name + '_bn', reuse) if with_bn else conv2d\n",
        "\n",
        "\n",
        "def depthwise_conv2d(input, depth_multiplier, name, is_training, kernel_size,\n",
        "                     reuse=None, with_bn=True, activation=tf.nn.elu):\n",
        "    conv2d = tf.contrib.layers.separable_conv2d(input, num_outputs=None, kernel_size=kernel_size, padding='VALID',\n",
        "                                                activation_fn=activation,\n",
        "                                                depth_multiplier=depth_multiplier,\n",
        "                                                weights_initializer=tf.glorot_normal_initializer(),\n",
        "                                                weights_regularizer=tf.contrib.layers.l2_regularizer(scale=1.0),\n",
        "                                                biases_initializer=None if with_bn else tf.zeros_initializer(),\n",
        "                                                biases_regularizer=None if with_bn else tf.contrib.layers.l2_regularizer(\n",
        "                                                    scale=1.0),\n",
        "                                                reuse=reuse, scope=name)\n",
        "    return batch_normalization(conv2d, is_training, name + '_bn', reuse) if with_bn else conv2d\n",
        "\n",
        "\n",
        "def conv2d(input, output, name, is_training, kernel_size,\n",
        "           reuse=None, with_bn=True, activation=tf.nn.elu):\n",
        "    conv2d = tf.layers.conv2d(input, output, kernel_size=kernel_size, strides=(1, 1), padding='VALID',\n",
        "                              activation=activation,\n",
        "                              kernel_initializer=tf.glorot_normal_initializer(),\n",
        "                              kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=1.0),\n",
        "                              reuse=reuse, name=name, use_bias=not with_bn)\n",
        "    return batch_normalization(conv2d, is_training, name + '_bn', reuse) if with_bn else conv2d\n",
        "\n",
        "\n",
        "def dense(input, output, name, is_training, reuse=None, with_bn=True, activation=tf.nn.elu):\n",
        "    dense = tf.layers.dense(input, units=output, activation=activation,\n",
        "                            kernel_initializer=tf.glorot_normal_initializer(),\n",
        "                            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=1.0),\n",
        "                            reuse=reuse, name=name, use_bias=not with_bn)\n",
        "    return batch_normalization(dense, is_training, name + '_bn', reuse) if with_bn else dense\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld2TLx6z7uqA"
      },
      "source": [
        "def get_xforms(xform_num, order='rxyz'):\n",
        "    xforms = np.empty(shape=(xform_num, 3, 3))\n",
        "    rotations = np.empty(shape=(xform_num, 3, 3))\n",
        "    for i in range(xform_num):\n",
        "        rx = rotation_angle(0, 'u')\n",
        "        ry = rotation_angle(0, 'u')\n",
        "        rz = rotation_angle(0, 'u')\n",
        "        rotation = euler2mat(rx, ry, rz, order)\n",
        "\n",
        "        sx = scaling_factor(0.0, 'g')\n",
        "        sy = scaling_factor(0.0, 'g')\n",
        "        sz = scaling_factor(0.0, 'g')\n",
        "        scaling = np.diag([sx, sy, sz])\n",
        "\n",
        "        xforms[i, :] = scaling * rotation\n",
        "        rotations[i, :] = rotation\n",
        "    return xforms, rotations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keF1dWkzAayz"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    summaries_op = tf.summary.merge_all('train')\n",
        "    summaries_val_op = tf.summary.merge_all('val')\n",
        "    summary_writer = tf.summary.FileWriter(folder_summary, sess.graph)\n",
        "\n",
        "    sess.run(init_op)\n",
        "\n",
        "    # Load the model\n",
        "    if args.load_ckpt is not None:\n",
        "        saver.restore(sess, args.load_ckpt)\n",
        "        print('{}-Checkpoint loaded from {}!'.format(datetime.now(), args.load_ckpt))\n",
        "    else:\n",
        "        latest_ckpt = tf.train.latest_checkpoint(folder_ckpt)\n",
        "        #latest_ckpt = '/content/drive/MyDrive/Colab_Notebooks/PointCNN/Train/npoint_2048_batch_16_epoch_401_learn_01_decay_100000_decaterate_035/ckpts_fold_2/iter-14500'\n",
        "        if latest_ckpt:\n",
        "            print('{}-Found checkpoint {}'.format(datetime.now(), latest_ckpt))\n",
        "            saver.restore(sess, latest_ckpt)\n",
        "            print('{}-Checkpoint loaded from {} (Iter {})'.format(\n",
        "                datetime.now(), latest_ckpt, sess.run(global_step)))\n",
        "\n",
        "    for batch_idx_train in range(batch_num):\n",
        "        ######################################################################\n",
        "        # Validation\n",
        "        if (batch_idx_train % step_val == 0 and (batch_idx_train != 0 or args.load_ckpt is not None)) \\\n",
        "                or batch_idx_train == batch_num - 1:\n",
        "            filename_ckpt = os.path.join(folder_ckpt, 'iter')\n",
        "            saver.save(sess, filename_ckpt, global_step=global_step)\n",
        "            print('{}-Checkpoint saved to {}!'.format(datetime.now(), filename_ckpt))\n",
        "\n",
        "            sess.run(reset_metrics_op)\n",
        "            for batch_val_idx in range(batch_num_val):\n",
        "                start_idx = batch_size * batch_val_idx\n",
        "                end_idx = min(start_idx + batch_size, num_val)\n",
        "                batch_size_val = end_idx - start_idx\n",
        "                points_batch = data_val[start_idx:end_idx, ...]\n",
        "                points_num_batch = data_num_val[start_idx:end_idx, ...]\n",
        "                labels_batch = label_val[start_idx:end_idx, ...].astype(int)\n",
        "                weights_batch = np.array(label_weights_list)[labels_batch]\n",
        "\n",
        "                xforms_np, rotations_np = get_xforms(batch_size_val,\n",
        "                                                        order=setting.rotation_order)\n",
        "                sess.run([loss_mean_update_op, t_1_acc_update_op, t_1_per_class_acc_update_op],\n",
        "                         feed_dict={\n",
        "                             pts_fts: points_batch,\n",
        "                             indices: get_indices(batch_size_val, sample_num, points_num_batch),\n",
        "                             xforms: xforms_np,\n",
        "                             rotations: rotations_np,\n",
        "                             jitter_range: np.array([jitter_val]),\n",
        "                             labels_seg: labels_batch,\n",
        "                             labels_weights: weights_batch,\n",
        "                             is_training: False,\n",
        "                         })\n",
        "            loss_val, t_1_acc_val, t_1_per_class_acc_val, summaries_val, step = sess.run(\n",
        "                [loss_mean_op, t_1_acc_op, t_1_per_class_acc_op, summaries_val_op, global_step])\n",
        "            summary_writer.add_summary(summaries_val, step)\n",
        "            print('{}-[Val  ]-Average:      Loss: {:.4f}  T-1 Acc: {:.4f}  T-1 mAcc: {:.4f}'\n",
        "                  .format(datetime.now(), loss_val, t_1_acc_val, t_1_per_class_acc_val))\n",
        "            sys.stdout.flush()\n",
        "        ######################################################################\n",
        "\n",
        "        ######################################################################\n",
        "        # Training\n",
        "        start_idx = (batch_size * batch_idx_train) % num_train\n",
        "        end_idx = min(start_idx + batch_size, num_train)\n",
        "        batch_size_train = end_idx - start_idx\n",
        "        points_batch = data_train[start_idx:end_idx, ...]\n",
        "        points_num_batch = data_num_train[start_idx:end_idx, ...]\n",
        "        labels_batch = label_train[start_idx:end_idx, ...].astype(int)\n",
        "        weights_batch = np.array(label_weights_list)[labels_batch]\n",
        "\n",
        "        if start_idx + batch_size_train == num_train:\n",
        "            data_train, data_num_train, label_train = \\\n",
        "                data_utils.grouped_shuffle([data_train, data_num_train, label_train])\n",
        "\n",
        "        offset = int(random.gauss(0, sample_num * setting.sample_num_variance))\n",
        "        offset = max(offset, int(-sample_num * setting.sample_num_clip))\n",
        "        offset = min(offset, int(sample_num * setting.sample_num_clip))\n",
        "        sample_num_train = sample_num + offset\n",
        "        xforms_np, rotations_np = get_xforms(batch_size_train,\n",
        "                                                order=setting.rotation_order)\n",
        "        \n",
        "        sess.run(reset_metrics_op)\n",
        "        sess.run([train_op, loss_mean_update_op, t_1_acc_update_op, t_1_per_class_acc_update_op],\n",
        "                 feed_dict={\n",
        "                     pts_fts: points_batch,\n",
        "                     indices: get_indices(batch_size_train, sample_num_train, points_num_batch),\n",
        "                     xforms: xforms_np,\n",
        "                     rotations: rotations_np,\n",
        "                     jitter_range: np.array([jitter]),\n",
        "                     labels_seg: labels_batch,\n",
        "                     labels_weights: weights_batch,\n",
        "                     is_training: True,\n",
        "                 })\n",
        "        if batch_idx_train % 10 == 0:\n",
        "            loss, t_1_acc, t_1_per_class_acc, summaries, step = sess.run([loss_mean_op,\n",
        "                                                                    t_1_acc_op,\n",
        "                                                                    t_1_per_class_acc_op,\n",
        "                                                                    summaries_op,\n",
        "                                                                    global_step])\n",
        "            summary_writer.add_summary(summaries, step)\n",
        "            print('{}-[Train]-Iter: {:06d}  Loss: {:.4f}  T-1 Acc: {:.4f}  T-1 mAcc: {:.4f}'\n",
        "                  .format(datetime.now(), step, loss, t_1_acc, t_1_per_class_acc))\n",
        "            sys.stdout.flush()\n",
        "        ######################################################################\n",
        "    print('{}-Done!'.format(datetime.now()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzpCnCcM99J9"
      },
      "source": [
        "label_weights_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpUFJQes1bNw"
      },
      "source": [
        "**Testing Process**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl8l67jpiGyb"
      },
      "source": [
        "root_folder = '/content/drive/MyDrive/Colab_Notebooks/PointCNN/predict/npoint_1300'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UeBFisaxk3g"
      },
      "source": [
        "folder_predict_fold_1 = os.path.join(root_folder, 'predict_fold_4_Patient')\n",
        "if not os.path.exists(folder_predict_fold_1):\n",
        "    os.makedirs(folder_predict_fold_1)\n",
        "\n",
        "#folder_true_fold_1 = os.path.join(root_folder, 'true_fold_3')\n",
        "#if not os.path.exists(folder_true_fold_1):\n",
        "#    os.makedirs(folder_true_fold_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Bmah0S0RTxo",
        "outputId": "0b567a2a-2ece-4b81-8cb1-3cf6f8c354a3"
      },
      "source": [
        "parser1 = argparse.ArgumentParser()\n",
        "parser1.add_argument('--category', '-c', help='Path to category list file (.txt)')\n",
        "\n",
        "parser1.add_argument('--load_ckpt', default='/content/drive/MyDrive/Colab_Notebooks/PointCNN/ckpts_fold_1', help='Path to a check point file for load')\n",
        "parser1.add_argument('--repeat_num', help='Repeat number', type=int, default=1)\n",
        "parser1.add_argument('--sample_num', help='Point sample num', type=int, default=1300)\n",
        "parser1.add_argument('--model', default='pointcnn_seg2', help='Model to use')\n",
        "\n",
        "parser1.add_argument('--save_ply', help='Save results as ply', action='store_true')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreTrueAction(option_strings=['--save_ply'], dest='save_ply', nargs=0, const=True, default=False, type=None, choices=None, help='Save results as ply', metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni9Urge0iRnU"
      },
      "source": [
        "args1 = parser1.parse_args(args=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Fk6EfWM7GsO"
      },
      "source": [
        "Train & Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDZuWS9ZiHlz"
      },
      "source": [
        "point_data = np.load('/content/drive/MyDrive/Colab_Notebooks/PointCNN/PartSegmentationData_data.npy')\n",
        "point_label = np.load('/content/drive/MyDrive/Colab_Notebooks/PointCNN/PartSegmentationData_label.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKm2amPeigrP"
      },
      "source": [
        "test_index = np.arange(666*0,666*1)\n",
        "for_train_index = np.array([i for i in range(len(point_data)) if i not in test_index])\n",
        "#np.random.shuffle(for_train_index)\n",
        "train_index = for_train_index[0:1920]\n",
        "valid_index = for_train_index[1920:1998]\n",
        "\n",
        "data_num = np.full((2664), 1024)\n",
        "# Prepare inputs\n",
        "data = point_data[test_index,0:1024,:]\n",
        "data_num = data_num[test_index,]\n",
        "label = point_label[test_index,0:1024]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce6YQCxzE57z"
      },
      "source": [
        "Patient Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHpZJGjcE3jR"
      },
      "source": [
        "point_data = np.load('/content/drive/MyDrive/Colab_Notebooks/PointCNN/PartSegmentationData_PatientData_Norm_2_Kinect1234_MRI1243.npy')\n",
        "data_num = np.full((16), 1300)\n",
        "# Prepare inputs\n",
        "data = point_data[:,0:1300,:]\n",
        "data_num = data_num[:,]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84kIfnV207b9"
      },
      "source": [
        "model = importlib.import_module(args1.model)\n",
        "\n",
        "batch_num = data.shape[0]\n",
        "max_point_num = data.shape[1]\n",
        "\n",
        "batch_size = args1.repeat_num * math.ceil(data.shape[1] / setting.sample_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKTv3qDno5bZ",
        "outputId": "0ca9329d-9cec-4e6e-b46a-6ca8631c9ec4"
      },
      "source": [
        "print('{}-{:d} testing batches.'.format(datetime.now(), batch_num))\n",
        "\n",
        "######################################################################\n",
        "# Placeholders\n",
        "indices = tf.placeholder(tf.int32, shape=(batch_size, None, 2), name=\"indices\")\n",
        "is_training = tf.placeholder(tf.bool, name='is_training')\n",
        "pts_fts = tf.placeholder(tf.float32, shape=(None, max_point_num, setting.data_dim), name='pts_fts')\n",
        "######################################################################\n",
        "\n",
        "######################################################################\n",
        "pts_fts_sampled = tf.gather_nd(pts_fts, indices=indices, name='pts_fts_sampled')\n",
        "if setting.data_dim > 3:\n",
        "    points_sampled, features_sampled = tf.split(pts_fts_sampled,\n",
        "                                                [3, setting.data_dim - 3],\n",
        "                                                axis=-1,\n",
        "                                                name='split_points_features')\n",
        "    if not setting.use_extra_features:\n",
        "        features_sampled = None\n",
        "else:\n",
        "    points_sampled = pts_fts_sampled\n",
        "    features_sampled = None\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-11-23 09:06:51.139557-16 testing batches.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7VGMIW30mL6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18d87fe7-2c87-4e09-8e4e-297347b5e584"
      },
      "source": [
        "net = model.Net(points_sampled, features_sampled, is_training, setting)\n",
        "logits = net.logits\n",
        "probs_op = tf.nn.softmax(logits, name='probs')\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "parameter_num = np.sum([np.prod(v.shape.as_list()) for v in tf.trainable_variables()])\n",
        "print('{}-Parameter number: {:d}.'.format(datetime.now(), parameter_num))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /content/drive/.shortcut-targets-by-id/1cHCgIPih8muLnTVO67gyq4Ml30OLbACX/PointCNN/pointfly.py:124: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /content/drive/.shortcut-targets-by-id/1cHCgIPih8muLnTVO67gyq4Ml30OLbACX/PointCNN/pointfly.py:144: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/.shortcut-targets-by-id/1cHCgIPih8muLnTVO67gyq4Ml30OLbACX/PointCNN/pointfly.py:347: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /content/drive/.shortcut-targets-by-id/1cHCgIPih8muLnTVO67gyq4Ml30OLbACX/PointCNN/pointfly.py:303: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "WARNING:tensorflow:From /content/drive/.shortcut-targets-by-id/1cHCgIPih8muLnTVO67gyq4Ml30OLbACX/PointCNN/pointfly.py:339: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /content/drive/.shortcut-targets-by-id/1cHCgIPih8muLnTVO67gyq4Ml30OLbACX/PointCNN/pointfly.py:315: separable_conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.SeparableConv2D` instead.\n",
            "WARNING:tensorflow:From /content/drive/.shortcut-targets-by-id/1cHCgIPih8muLnTVO67gyq4Ml30OLbACX/PointCNN/pointcnn2.py:158: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "2021-11-23 09:07:00.219997-Parameter number: 8326770.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s2nNM9fla0W"
      },
      "source": [
        "sample_num = setting.sample_num"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO3PkzFW6Mfi"
      },
      "source": [
        "sample_num = 1300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d54HlbR49mG"
      },
      "source": [
        "import scipy.io"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYCFantbgEQc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6f13e0b-5c7a-4b1d-d4a7-ad7f82701f1c"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    # Load the model\n",
        "    saver.restore(sess, '/content/drive/MyDrive/Colab_Notebooks/PointCNN/Train/npoint_2048_batch_16_epoch_401_learn_01_decay_200000_decaterate_035/ckpts_fold_1/iter-30000')\n",
        "    print('{}-Checkpoint loaded from {}!'.format(datetime.now(), args1.load_ckpt))\n",
        "\n",
        "    indices_batch_indices = np.tile(np.reshape(np.arange(batch_size), (batch_size, 1, 1)), (1, sample_num, 1))\n",
        "    for batch_idx in range(batch_num):\n",
        "        points_batch = data[[batch_idx] * batch_size, ...]\n",
        "        #object_label = label[batch_idx]\n",
        "        point_num = data_num[batch_idx]\n",
        "        #category = category_list[object_label][0]\n",
        "        #label_start, label_end = category_range[category]\n",
        "\n",
        "        tile_num = math.ceil((sample_num * batch_size) / point_num)\n",
        "        indices_shuffle = np.tile(np.arange(point_num), tile_num)[0:sample_num * batch_size]\n",
        "        np.random.shuffle(indices_shuffle)\n",
        "        indices_batch_shuffle = np.reshape(indices_shuffle, (batch_size, sample_num, 1))\n",
        "        indices_batch = np.concatenate((indices_batch_indices, indices_batch_shuffle), axis=2)\n",
        "\n",
        "        probs = sess.run([probs_op],\n",
        "                            feed_dict={\n",
        "                                pts_fts: points_batch,\n",
        "                                indices: indices_batch,\n",
        "                                is_training: False,\n",
        "                            })\n",
        "        probs_2d = np.reshape(probs, (sample_num * batch_size, -1))\n",
        "        predictions = [(-1, 0.0)] * point_num\n",
        "        for idx in range(sample_num * batch_size):\n",
        "            point_idx = indices_shuffle[idx]\n",
        "            probs = probs_2d[idx, :]\n",
        "            confidence = np.amax(probs)\n",
        "            seg_idx = np.argmax(probs)\n",
        "            if confidence > predictions[point_idx][1]:\n",
        "                predictions[point_idx] = (seg_idx, confidence)\n",
        "\n",
        "        \n",
        "        labels = []\n",
        "        for seg_idx, _ in predictions:\n",
        "            labels.append(seg_idx)\n",
        "        \n",
        "        arr = labels\n",
        "        scipy.io.savemat('/content/drive/MyDrive/Colab_Notebooks/PointCNN/predict/npoint_1300/predict_fold_1_Patient/'+ str(batch_idx) + '.mat', mdict={'arr': arr})\n",
        "        labels = []\n",
        "        \n",
        "        #with open('/content/drive/MyDrive/Colab_Notebooks/PointCNN/output_seg_predict/'+ str(batch_idx) + '.txt', 'w') as file_seg:\n",
        "        #    for seg_idx, _ in predictions:\n",
        "        #        file_seg.write('%d\\n' % (seg_idx))\n",
        "        #        labels.append(seg_idx)\n",
        "        #    file_seg.close() \n",
        "\n",
        "        sys.stdout.flush()\n",
        "        ######################################################################\n",
        "    print('{}-Done!'.format(datetime.now()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/Colab_Notebooks/PointCNN/Train/npoint_2048_batch_16_epoch_401_learn_01_decay_200000_decaterate_035/ckpts_fold_1/iter-30000\n",
            "2021-11-23 09:08:01.964057-Checkpoint loaded from /content/drive/MyDrive/Colab_Notebooks/PointCNN/ckpts_fold_1!\n",
            "2021-11-23 09:08:04.549722-Done!\n"
          ]
        }
      ]
    }
  ]
}